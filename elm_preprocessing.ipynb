{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# ELM fNIRS Data Preprocessing\n",
    "\n",
    "This notebook preprocesses fNIRS data from the ELM study.\n",
    "\n",
    "**Use only the first code cell** to execute this notebook automatically for multiple subjects in a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-processing",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": "import os\nimport json\nimport sys\nfrom pathlib import Path\n\ninput_base = Path(\"/Users/saewonchung/Desktop/ELM_MW_data_analysis/ELM_filtered_data\")\noutput_base = Path(\"/Users/saewonchung/Desktop/ELM_MW_data_analysis/ELM_preprocessed\")\n\n# Auto-discover all input directories with .tri files\ninput_dirs = []\nfor date_folder in sorted(input_base.iterdir()):\n    if not date_folder.is_dir():\n        continue\n    for session_folder in sorted(date_folder.iterdir()):\n        if not session_folder.is_dir():\n            continue\n        # Check if .tri file exists\n        if list(session_folder.glob(\"*.tri\")):\n            relative_path = f\"{date_folder.name}/{session_folder.name}\"\n            input_dirs.append(relative_path)\n\nprint(f\"Found {len(input_dirs)} datasets with .tri files\")\nprint(\"First 5:\", input_dirs[:5])\n\n# Trigger configuration for segmentation\nsegment_by_triggers = {\n    \"2.0\": \"Video/Begin\",\n    \"3.0\": \"Video/End\",\n    \"0.0\": \"Video/TaskEnd\"\n}\n\n# ============================================================================\n# Code for reprocessing only missing subjects\n# After adjusting duration tolerance, reprocess only 14 subjects missing\n# Zima/Splitscreen label files\n# Target subjects: 40, 41, 42, 43, 44, 45, 46, 47, 50, 55, 56, 57, 109, 113\n# (sub-53 is unrecoverable due to trigger recording error)\n#\n# Usage: Set REPROCESS_MISSING_ONLY = True and run the first cell\n# ============================================================================\nREPROCESS_MISSING_ONLY = True  # Set to True to reprocess only missing subjects\n\nmissing_subjects = ['40', '41', '42', '43', '44', '45', '46', '47', '50', '55', '56', '57', '109', '113']\n\nif REPROCESS_MISSING_ONLY:\n    # Filter input_dirs to only include missing subjects\n    def get_subject_id(input_dir_path):\n        \"\"\"Extract subject ID from description.json\"\"\"\n        desc_files = list(input_dir_path.glob(\"*_description.json\"))\n        if desc_files:\n            with open(desc_files[0], 'r') as f:\n                desc = json.load(f)\n                return desc.get('subject', '')\n        return ''\n    \n    filtered_dirs = []\n    for rel_path in input_dirs:\n        full_path = input_base / rel_path\n        subj_id = get_subject_id(full_path)\n        # Remove leading zeros for comparison (e.g., \"055\" -> \"55\")\n        subj_id_clean = subj_id.lstrip('0') or '0'\n        if subj_id_clean in missing_subjects or subj_id in missing_subjects:\n            filtered_dirs.append(rel_path)\n            print(f\"  ‚Üí Will reprocess: {rel_path} (subject {subj_id})\")\n    \n    input_dirs = filtered_dirs\n    print(f\"\\nüîÑ Reprocessing {len(input_dirs)} missing subjects only\")\n\nif 'INPUT_DIR' not in os.environ:\n    import subprocess\n\n    # Get notebook path\n    from IPython import get_ipython\n    ip = get_ipython()\n    if '__vsc_ipynb_file__' in ip.user_ns:\n        nb_path = ip.user_ns['__vsc_ipynb_file__']\n    else:\n        import ipynbname\n        nb_path = ipynbname.path()\n\n    # For each input directory, submit a subprocess to run nbconvert on this notebook\n    for target_input_dir in input_dirs:\n        date_folder = target_input_dir.split('/')[0]\n        session_folder = target_input_dir.split('/')[1]\n        full_input_dir = input_base / date_folder / session_folder\n\n        if not full_input_dir.exists():\n            print(f\"‚ö†Ô∏è  Skipping: {full_input_dir} does not exist.\")\n            continue\n\n        # Check if tri file is present\n        if not list(full_input_dir.glob(\"*.tri\")):\n            print(f\"‚ùå Skipping: No .tri file found in {full_input_dir}\")\n            continue\n\n        env = os.environ.copy()\n        env[\"INPUT_DIR\"] = str(full_input_dir)\n\n        output_name = f\"{session_folder}.ipynb\"\n\n        try:\n            print(f\"\\n{'='*60}\")\n            print(f\"Processing: {target_input_dir}\")\n            print(f\"{'='*60}\")\n            # Use python -m jupyter instead of jupyter command directly\n            subprocess.run([\n                sys.executable, \"-m\", \"jupyter\", \"nbconvert\",\n                \"--to\", \"notebook\",\n                \"--execute\", str(nb_path),\n                \"--output\", output_name,\n                \"--output-dir\", str(output_base)\n            ], env=env, check=True)\n            print(f\"‚úÖ Success: {target_input_dir}\")\n        except subprocess.CalledProcessError as e:\n            print(f\"‚ùå Notebook execution failed for {target_input_dir}: {e}\")"
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing-header",
   "metadata": {},
   "source": [
    "# Preprocessing Pipeline\n",
    "\n",
    "Code adapted from https://mne.tools/stable/auto_tutorials/preprocessing/70_fnirs_processing.html\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "if 'INPUT_DIR' not in os.environ:\n",
    "    raise Exception('Missing INPUT_DIR variable! Need to run using nbconvert; see first code cell')\n",
    "\n",
    "from itertools import compress\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mne\n",
    "import mne_nirs\n",
    "\n",
    "input_dir = os.environ['INPUT_DIR']\n",
    "\n",
    "input_path = Path(input_dir)\n",
    "raw_intensity = mne.io.read_raw_nirx(input_path, verbose=True).load_data()\n",
    "\n",
    "subj = raw_intensity.info['subject_info']['his_id']\n",
    "\n",
    "# Extract session identifier from input directory (e.g., \"2025-03-12_001\")\n",
    "session_id = input_path.name\n",
    "\n",
    "# Check if accelerometer is available\n",
    "config_file = list(input_path.glob(\"*_config.json\"))[0]\n",
    "with open(config_file, 'r') as f:\n",
    "    config = json.load(f)\n",
    "has_accelerometer = config.get('use_accelerometer', False)\n",
    "print(f\"Dataset has accelerometer: {has_accelerometer}\")\n",
    "print(f\"Session ID: {session_id}\")\n",
    "\n",
    "# Create the full output directory\n",
    "output_dir = output_base / f'sub-{subj}'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-header",
   "metadata": {},
   "source": [
    "## Step 1: Compute Optical Density Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-density",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Plot raw intensity time series\n",
    "raw_intensity.plot(n_channels=len(raw_intensity.ch_names), duration=500, show_scrollbars=False)\n",
    "\n",
    "# Convert from raw intensity to optical density and plot\n",
    "raw_od = mne.preprocessing.nirs.optical_density(raw_intensity)\n",
    "raw_od.plot(n_channels=len(raw_od.ch_names), duration=500, show_scrollbars=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-header",
   "metadata": {},
   "source": [
    "## Step 2: Signal Quality Assessment (SCI)\n",
    "\n",
    "Identify and filter out channels with poor signal quality using the Scalp Coupling Index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sci-visualization",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize original SCI distribution\n",
    "sci = mne.preprocessing.nirs.scalp_coupling_index(raw_od)\n",
    "fig, ax = plt.subplots(layout=\"constrained\")\n",
    "ax.hist(sci)\n",
    "ax.set(xlabel=\"Scalp Coupling Index\", ylabel=\"Count\", xlim=[0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sci-filtering",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Filter out bad channels (SCI < 0.5)\n",
    "od = raw_od.copy()\n",
    "od.info[\"bads\"] = list(compress(od.ch_names, sci < 0.5))\n",
    "bad_channels = list(compress(od.ch_names, sci < 0.5))\n",
    "od.drop_channels(bad_channels)\n",
    "print(f\"Dropped bad channels based on SCI < 0.5: {bad_channels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sci-verification",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize new SCI distribution\n",
    "sci_clean = mne.preprocessing.nirs.scalp_coupling_index(od)\n",
    "fig, ax = plt.subplots(layout=\"constrained\")\n",
    "ax.hist(sci_clean, bins=20)\n",
    "ax.set(xlabel=\"Scalp Coupling Index (after removal of bad channels)\", ylabel=\"Count\", xlim=[0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-header",
   "metadata": {},
   "source": [
    "## Step 3: Signal Enhancement - Short Channel Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-channel-regression",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Regress out short channels to remove systemic noise\n",
    "od = mne_nirs.signal_enhancement.short_channel_regression(od)\n",
    "od.plot(n_channels=len(od.ch_names), duration=500, show_scrollbars=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-header",
   "metadata": {},
   "source": [
    "## Step 4: Motion Artifact Correction - TDDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tddr",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Apply Temporal Derivative Distribution Repair to remove motion artifacts\n",
    "od = mne.preprocessing.nirs.temporal_derivative_distribution_repair(od)\n",
    "od.plot(n_channels=len(od.ch_names), duration=500, show_scrollbars=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5-header",
   "metadata": {},
   "source": [
    "## Step 5: Motion Artifact Correction - Accelerometer (if available)\n",
    "\n",
    "TODO: Implement accelerometer-based motion correction for datasets with `use_accelerometer: true`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accelerometer-correction",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "if has_accelerometer:\n",
    "    print(\"‚ö†Ô∏è  Accelerometer data detected but correction not yet implemented\")\n",
    "    print(\"TODO: Add accelerometer-based motion artifact correction\")\n",
    "    # Future implementation here\n",
    "else:\n",
    "    print(\"No accelerometer data - skipping accelerometer-based correction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6-header",
   "metadata": {},
   "source": [
    "## Step 6: Convert to Haemoglobin Concentration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beer-lambert",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Convert from optical density to haemoglobin using Beer-Lambert law\n",
    "raw_haemo = mne.preprocessing.nirs.beer_lambert_law(od, ppf=0.1)\n",
    "raw_haemo = mne_nirs.channels.get_long_channels(raw_haemo)\n",
    "\n",
    "# Plot haemoglobin time series\n",
    "raw_haemo.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step7-header",
   "metadata": {},
   "source": [
    "## Step 7: Signal Filtering - Remove Heart Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filtering",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "haemo = raw_haemo.copy()\n",
    "haemo.filter(0.05, 0.7, h_trans_bandwidth=0.2, l_trans_bandwidth=0.02)\n",
    "\n",
    "# Visualize power spectral density before and after filtering\n",
    "for when, _haemo in dict(Before=raw_haemo, After=haemo).items():\n",
    "    fig = _haemo.compute_psd().plot(\n",
    "        average=True, amplitude=False, picks=\"data\", exclude=\"bads\"\n",
    "    )\n",
    "    fig.suptitle(f\"{when} filtering\", weight=\"bold\", size=\"x-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step8-header",
   "metadata": {},
   "source": [
    "## Step 8: Trigger-Based Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trigger-segmentation",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Rename triggers according to segment_by_triggers dictionary\n",
    "haemo.annotations.rename(segment_by_triggers)\n",
    "\n",
    "# Keep only the annotations we want to segment by\n",
    "wanted_annotations = list(segment_by_triggers.values())\n",
    "unwanted_annotations = np.nonzero(~np.isin(haemo.annotations.description, wanted_annotations))\n",
    "haemo.annotations.delete(unwanted_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hrf-shift",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "events, event_dict = mne.events_from_annotations(haemo)\n",
    "\n",
    "# Make each event unique\n",
    "event_desc = {v: k for k, v in event_dict.items()}\n",
    "original_event_ids = []\n",
    "for i, event in enumerate(events):\n",
    "    original_event_ids.append(event_desc[event[2]])\n",
    "    event[2] = i + 1\n",
    "\n",
    "# Apply HRF delay shift (6 seconds)\n",
    "shift_seconds_for_hrf_delay = 6.0\n",
    "events_hrf_shifted = mne.event.shift_time_events(\n",
    "    events, ids=None, tshift=shift_seconds_for_hrf_delay, sfreq=haemo.info[\"sfreq\"]\n",
    ")\n",
    "\n",
    "# Create annotations from shifted events\n",
    "annot_from_events = mne.annotations_from_events(\n",
    "    events=events_hrf_shifted,\n",
    "    sfreq=haemo.info[\"sfreq\"],\n",
    "    orig_time=haemo.info[\"meas_date\"],\n",
    "    first_samp=haemo.first_samp\n",
    ")\n",
    "\n",
    "# Set durations to span until next annotation\n",
    "mapping = {}\n",
    "for i, annot in enumerate(annot_from_events):\n",
    "    if i == len(annot_from_events) - 1:\n",
    "        continue\n",
    "    duration = annot_from_events.onset[i + 1] - annot[\"onset\"]\n",
    "    mapping[annot[\"description\"]] = duration\n",
    "annot_from_events.set_durations(mapping, verbose=True)\n",
    "\n",
    "# Restore original event IDs\n",
    "annot_from_events.rename(\n",
    "    {str(i + 1): original_event_ids[i] for i in range(len(original_event_ids))}\n",
    ")\n",
    "\n",
    "# Segment the data by annotations\n",
    "haemo_segments = haemo.crop_by_annotations(annot_from_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step9-header",
   "metadata": {},
   "source": [
    "## Step 9: Export to CSV\n",
    "\n",
    "Save full and segmented data using BIDS-like naming conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-csv",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Video duration to name mapping\n",
    "# NOTE: Original exact matching caused 15 subjects to miss video labels due to trigger timing variance\n",
    "# Affected subjects: sub-40~47, 50, 53, 55~57, 109, 113 (durations like 507, 504, 144 instead of 508, 145)\n",
    "# Additionally, 5 subjects have no source data: sub-1, 49, 63 (recorded as P63), 74, 77\n",
    "\n",
    "# Original exact matching (kept for reference):\n",
    "# video_duration_to_name = {\n",
    "#     508: 'Zima',\n",
    "#     145: 'Splitscreen'\n",
    "# }\n",
    "\n",
    "# Updated: Use tolerance-based matching to handle trigger timing variance\n",
    "def get_video_name(duration, tolerance=5):\n",
    "    \"\"\"Match video duration with tolerance for trigger timing variance.\"\"\"\n",
    "    if abs(duration - 508) <= tolerance:\n",
    "        return 'Zima'\n",
    "    elif abs(duration - 145) <= tolerance:\n",
    "        return 'Splitscreen'\n",
    "    return None\n",
    "\n",
    "# Export full preprocessed data with session ID\n",
    "haemo.to_data_frame().to_csv(output_dir / f\"sub-{subj}_ses-{session_id}_desc-preproc_haemo.csv\", index=False)\n",
    "annot_from_events.to_data_frame().to_csv(output_dir / f\"sub-{subj}_ses-{session_id}_annotations.csv\", index=False)\n",
    "\n",
    "# Export segmented data\n",
    "segment_index = 0\n",
    "for i, segment in enumerate(haemo_segments):\n",
    "    duration = int(annot_from_events.duration[i])\n",
    "    if duration > 0:\n",
    "        segment_index += 1\n",
    "        task_label = annot_from_events.description[i].split(\"/\")[0]\n",
    "        \n",
    "        # Check if this duration corresponds to a named video (with tolerance)\n",
    "        video_name = get_video_name(duration)\n",
    "        if video_name:\n",
    "            print(f\"Segment {segment_index}: duration={duration}s, video={video_name}\")\n",
    "            # Save with video name\n",
    "            segment.to_data_frame().to_csv(\n",
    "                output_dir / f\"sub-{subj}_ses-{session_id}_task-{task_label}_label-{video_name}_haemo.csv\",\n",
    "                index=False\n",
    "            )\n",
    "        \n",
    "        # Always save with acq number and duration\n",
    "        segment.to_data_frame().to_csv(\n",
    "            output_dir / f\"sub-{subj}_ses-{session_id}_task-{task_label}_acq-{segment_index}_dur-{duration}_desc-preproc_haemo.csv\",\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "print(f\"\\n‚úÖ Preprocessing complete for subject {subj}, session {session_id}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Exported {segment_index} segments\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}