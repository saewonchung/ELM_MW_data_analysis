{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# ELM fNIRS Data Preprocessing\n",
    "\n",
    "This notebook preprocesses fNIRS data from the ELM study.\n",
    "\n",
    "**Use only the first code cell** to execute this notebook automatically for multiple subjects in a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "batch-processing",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 118 datasets with .tri files\n",
      "First 5: ['2025-02-26/2025-02-26_002', '2025-03-03/2025-03-03_001', '2025-03-05/2025-03-05_001', '2025-03-05/2025-03-05_002', '2025-03-07/2025-03-07_001']\n",
      "  ‚Üí Will reprocess: 2025-05-19/2025-05-19_002 (subject 40)\n",
      "  ‚Üí Will reprocess: 2025-05-19/2025-05-19_003 (subject 40)\n",
      "  ‚Üí Will reprocess: 2025-05-22/2025-05-22_001 (subject 41)\n",
      "  ‚Üí Will reprocess: 2025-05-23/2025-05-23_001 (subject 42)\n",
      "  ‚Üí Will reprocess: 2025-05-26/2025-05-26_001 (subject 43)\n",
      "  ‚Üí Will reprocess: 2025-05-28/2025-05-28_001 (subject 44)\n",
      "  ‚Üí Will reprocess: 2025-05-29/2025-05-29_001 (subject 45)\n",
      "  ‚Üí Will reprocess: 2025-05-29/2025-05-29_002 (subject 46)\n",
      "  ‚Üí Will reprocess: 2025-06-26/2025-06-26_002 (subject 47)\n",
      "  ‚Üí Will reprocess: 2025-08-21/2025-08-21_001 (subject 50)\n",
      "  ‚Üí Will reprocess: 2025-08-25/2025-08-25_002 (subject 055)\n",
      "  ‚Üí Will reprocess: 2025-08-25/2025-08-25_003 (subject 056)\n",
      "  ‚Üí Will reprocess: 2025-08-27/2025-08-27_001 (subject 057)\n",
      "  ‚Üí Will reprocess: 2025-12-03/2025-12-03_001 (subject 109)\n",
      "  ‚Üí Will reprocess: 2025-12-05/2025-12-05_004 (subject 113)\n",
      "\n",
      "üîÑ Reprocessing 15 missing subjects only\n",
      "\n",
      "============================================================\n",
      "Processing: 2025-05-19/2025-05-19_002\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/ELM_MW_data_analysis/elm_preprocessing.ipynb to notebook\n",
      "[NbConvertApp] Writing 4439677 bytes to /Users/saewonchung/Desktop/ELM_MW_data_analysis/ELM_preprocessed/2025-05-19_002.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Success: 2025-05-19/2025-05-19_002\n",
      "\n",
      "============================================================\n",
      "Processing: 2025-05-19/2025-05-19_003\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/ELM_MW_data_analysis/elm_preprocessing.ipynb to notebook\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/Library/Frameworks/Python.framework/Versions/3.13/bin/jupyter-nbconvert\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/saewonchung/Library/Python/3.13/lib/python/site-packages/jupyter_core/application.py\"\u001b[0m, line \u001b[35m283\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31msuper().launch_instance\u001b[0m\u001b[1;31m(argv=argv, **kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/traitlets/config/application.py\"\u001b[0m, line \u001b[35m1075\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31mapp.start\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m420\u001b[0m, in \u001b[35mstart\u001b[0m\n",
      "    \u001b[31mself.convert_notebooks\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m597\u001b[0m, in \u001b[35mconvert_notebooks\u001b[0m\n",
      "    \u001b[31mself.convert_single_notebook\u001b[0m\u001b[1;31m(notebook_filename)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m563\u001b[0m, in \u001b[35mconvert_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.export_single_notebook\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources, input_buffer=input_buffer\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m487\u001b[0m, in \u001b[35mexport_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.exporter.from_filename\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources=resources\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m201\u001b[0m, in \u001b[35mfrom_filename\u001b[0m\n",
      "    return \u001b[31mself.from_file\u001b[0m\u001b[1;31m(f, resources=resources, **kw)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m220\u001b[0m, in \u001b[35mfrom_file\u001b[0m\n",
      "    return \u001b[31mself.from_notebook_node\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnbformat.read(file_stream, as_version=4), resources=resources, **kw\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nbconvert/exporters/notebook.py\"\u001b[0m, line \u001b[35m36\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31msuper().from_notebook_node\u001b[0m\u001b[1;31m(nb, resources, **kw)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31mself._preprocess\u001b[0m\u001b[1;31m(nb_copy, resources)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m353\u001b[0m, in \u001b[35m_preprocess\u001b[0m\n",
      "    nbc, resc = \u001b[31mpreprocessor\u001b[0m\u001b[1;31m(nbc, resc)\u001b[0m\n",
      "                \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nbconvert/preprocessors/base.py\"\u001b[0m, line \u001b[35m48\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return \u001b[31mself.preprocess\u001b[0m\u001b[1;31m(nb, resources)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m103\u001b[0m, in \u001b[35mpreprocess\u001b[0m\n",
      "    \u001b[31mself.preprocess_cell\u001b[0m\u001b[1;31m(cell, resources, index)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m124\u001b[0m, in \u001b[35mpreprocess_cell\u001b[0m\n",
      "    cell = self.execute_cell(cell, index, store_history=True)\n",
      "  File \u001b[35m\"/Users/saewonchung/Library/Python/3.13/lib/python/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m165\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    return \u001b[31mloop.run_until_complete\u001b[0m\u001b[1;31m(inner)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py\"\u001b[0m, line \u001b[35m719\u001b[0m, in \u001b[35mrun_until_complete\u001b[0m\n",
      "    return \u001b[31mfuture.result\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m1062\u001b[0m, in \u001b[35masync_execute_cell\u001b[0m\n",
      "    await self._check_raise_for_error(cell, cell_index, exec_reply)\n",
      "  File \u001b[35m\"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m918\u001b[0m, in \u001b[35m_check_raise_for_error\u001b[0m\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
      "\u001b[1;35mnbclient.exceptions.CellExecutionError\u001b[0m: \u001b[35mAn error occurred while executing the following cell:\n",
      "------------------\n",
      "# Rename triggers according to segment_by_triggers dictionary\n",
      "haemo.annotations.rename(segment_by_triggers)\n",
      "\n",
      "# Keep only the annotations we want to segment by\n",
      "wanted_annotations = list(segment_by_triggers.values())\n",
      "unwanted_annotations = np.nonzero(~np.isin(haemo.annotations.description, wanted_annotations))\n",
      "haemo.annotations.delete(unwanted_annotations)\n",
      "------------------\n",
      "\n",
      "\n",
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n",
      "\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Rename triggers according to segment_by_triggers dictionary\u001b[39;00m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mhaemo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegment_by_triggers\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Keep only the annotations we want to segment by\u001b[39;00m\n",
      "\u001b[32m      5\u001b[39m wanted_annotations = \u001b[38;5;28mlist\u001b[39m(segment_by_triggers.values())\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<decorator-gen-40>:12\u001b[39m, in \u001b[36mrename\u001b[39m\u001b[34m(self, mapping, verbose)\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/mne/annotations.py:751\u001b[39m, in \u001b[36mAnnotations.rename\u001b[39m\u001b[34m(self, mapping, verbose)\u001b[39m\n",
      "\u001b[32m    732\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Rename annotation description(s). Operates inplace.\u001b[39;00m\n",
      "\u001b[32m    733\u001b[39m \n",
      "\u001b[32m    734\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m    748\u001b[39m \u001b[33;03m.. versionadded:: 0.24.0\u001b[39;00m\n",
      "\u001b[32m    749\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n",
      "\u001b[32m    750\u001b[39m _validate_type(mapping, \u001b[38;5;28mdict\u001b[39m)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m751\u001b[39m \u001b[43m_check_dict_keys\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    752\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    753\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    754\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_key_source\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    755\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_description\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAnnotation description(s)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    756\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    757\u001b[39m \u001b[38;5;28mself\u001b[39m.description = np.array([\u001b[38;5;28mstr\u001b[39m(mapping.get(d, d)) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.description])\n",
      "\u001b[32m    758\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/mne/utils/check.py:879\u001b[39m, in \u001b[36m_check_dict_keys\u001b[39m\u001b[34m(mapping, valid_keys, key_description, valid_key_source)\u001b[39m\n",
      "\u001b[32m    874\u001b[39m     _is = \u001b[33m\"\u001b[39m\u001b[33mare\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mis\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m    875\u001b[39m     msg = (\n",
      "\u001b[32m    876\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey_description\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_is\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not present in \u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m    877\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_key_source\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m    878\u001b[39m     )\n",
      "\u001b[32m--> \u001b[39m\u001b[32m879\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[32m    881\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m mapping\n",
      "\n",
      "\u001b[31mValueError\u001b[39m: Invalid Annotation description(s) {'0.0'} is not present in data\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Notebook execution failed for 2025-05-19/2025-05-19_003: Command '['/Library/Developer/CommandLineTools/usr/bin/python3', '-m', 'jupyter', 'nbconvert', '--to', 'notebook', '--execute', '/Users/saewonchung/Desktop/ELM_MW_data_analysis/elm_preprocessing.ipynb', '--output', '2025-05-19_003.ipynb', '--output-dir', '/Users/saewonchung/Desktop/ELM_MW_data_analysis/ELM_preprocessed']' returned non-zero exit status 1.\n",
      "\n",
      "============================================================\n",
      "Processing: 2025-05-22/2025-05-22_001\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/ELM_MW_data_analysis/elm_preprocessing.ipynb to notebook\n",
      "[NbConvertApp] Writing 4107162 bytes to /Users/saewonchung/Desktop/ELM_MW_data_analysis/ELM_preprocessed/2025-05-22_001.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Success: 2025-05-22/2025-05-22_001\n",
      "\n",
      "============================================================\n",
      "Processing: 2025-05-23/2025-05-23_001\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/ELM_MW_data_analysis/elm_preprocessing.ipynb to notebook\n",
      "[NbConvertApp] Writing 4006190 bytes to /Users/saewonchung/Desktop/ELM_MW_data_analysis/ELM_preprocessed/2025-05-23_001.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Success: 2025-05-23/2025-05-23_001\n",
      "\n",
      "============================================================\n",
      "Processing: 2025-05-26/2025-05-26_001\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/ELM_MW_data_analysis/elm_preprocessing.ipynb to notebook\n",
      "[NbConvertApp] Writing 4463162 bytes to /Users/saewonchung/Desktop/ELM_MW_data_analysis/ELM_preprocessed/2025-05-26_001.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Success: 2025-05-26/2025-05-26_001\n",
      "\n",
      "============================================================\n",
      "Processing: 2025-05-28/2025-05-28_001\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/ELM_MW_data_analysis/elm_preprocessing.ipynb to notebook\n",
      "[NbConvertApp] Writing 4023065 bytes to /Users/saewonchung/Desktop/ELM_MW_data_analysis/ELM_preprocessed/2025-05-28_001.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Success: 2025-05-28/2025-05-28_001\n",
      "\n",
      "============================================================\n",
      "Processing: 2025-05-29/2025-05-29_001\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/ELM_MW_data_analysis/elm_preprocessing.ipynb to notebook\n",
      "[NbConvertApp] Writing 4553057 bytes to /Users/saewonchung/Desktop/ELM_MW_data_analysis/ELM_preprocessed/2025-05-29_001.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Success: 2025-05-29/2025-05-29_001\n",
      "\n",
      "============================================================\n",
      "Processing: 2025-05-29/2025-05-29_002\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/ELM_MW_data_analysis/elm_preprocessing.ipynb to notebook\n",
      "[NbConvertApp] Writing 3732113 bytes to /Users/saewonchung/Desktop/ELM_MW_data_analysis/ELM_preprocessed/2025-05-29_002.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Success: 2025-05-29/2025-05-29_002\n",
      "\n",
      "============================================================\n",
      "Processing: 2025-06-26/2025-06-26_002\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/ELM_MW_data_analysis/elm_preprocessing.ipynb to notebook\n",
      "[NbConvertApp] Writing 3538259 bytes to /Users/saewonchung/Desktop/ELM_MW_data_analysis/ELM_preprocessed/2025-06-26_002.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Success: 2025-06-26/2025-06-26_002\n",
      "\n",
      "============================================================\n",
      "Processing: 2025-08-21/2025-08-21_001\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/ELM_MW_data_analysis/elm_preprocessing.ipynb to notebook\n",
      "[NbConvertApp] Writing 3518283 bytes to /Users/saewonchung/Desktop/ELM_MW_data_analysis/ELM_preprocessed/2025-08-21_001.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Success: 2025-08-21/2025-08-21_001\n",
      "\n",
      "============================================================\n",
      "Processing: 2025-08-25/2025-08-25_002\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/ELM_MW_data_analysis/elm_preprocessing.ipynb to notebook\n",
      "[NbConvertApp] Writing 2948370 bytes to /Users/saewonchung/Desktop/ELM_MW_data_analysis/ELM_preprocessed/2025-08-25_002.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Success: 2025-08-25/2025-08-25_002\n",
      "\n",
      "============================================================\n",
      "Processing: 2025-08-25/2025-08-25_003\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/ELM_MW_data_analysis/elm_preprocessing.ipynb to notebook\n",
      "[NbConvertApp] Writing 3517581 bytes to /Users/saewonchung/Desktop/ELM_MW_data_analysis/ELM_preprocessed/2025-08-25_003.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Success: 2025-08-25/2025-08-25_003\n",
      "\n",
      "============================================================\n",
      "Processing: 2025-08-27/2025-08-27_001\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/ELM_MW_data_analysis/elm_preprocessing.ipynb to notebook\n",
      "[NbConvertApp] Writing 4293145 bytes to /Users/saewonchung/Desktop/ELM_MW_data_analysis/ELM_preprocessed/2025-08-27_001.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Success: 2025-08-27/2025-08-27_001\n",
      "\n",
      "============================================================\n",
      "Processing: 2025-12-03/2025-12-03_001\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/ELM_MW_data_analysis/elm_preprocessing.ipynb to notebook\n",
      "[NbConvertApp] Writing 4034686 bytes to /Users/saewonchung/Desktop/ELM_MW_data_analysis/ELM_preprocessed/2025-12-03_001.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Success: 2025-12-03/2025-12-03_001\n",
      "\n",
      "============================================================\n",
      "Processing: 2025-12-05/2025-12-05_004\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/ELM_MW_data_analysis/elm_preprocessing.ipynb to notebook\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Success: 2025-12-05/2025-12-05_004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Writing 4245564 bytes to /Users/saewonchung/Desktop/ELM_MW_data_analysis/ELM_preprocessed/2025-12-05_004.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "input_base = Path(\"/Users/saewonchung/Desktop/ELM_MW_data_analysis/ELM_filtered_data\")\n",
    "output_base = Path(\"/Users/saewonchung/Desktop/ELM_MW_data_analysis/ELM_preprocessed\")\n",
    "\n",
    "# Auto-discover all input directories with .tri files\n",
    "input_dirs = []\n",
    "for date_folder in sorted(input_base.iterdir()):\n",
    "    if not date_folder.is_dir():\n",
    "        continue\n",
    "    for session_folder in sorted(date_folder.iterdir()):\n",
    "        if not session_folder.is_dir():\n",
    "            continue\n",
    "        # Check if .tri file exists\n",
    "        if list(session_folder.glob(\"*.tri\")):\n",
    "            relative_path = f\"{date_folder.name}/{session_folder.name}\"\n",
    "            input_dirs.append(relative_path)\n",
    "\n",
    "print(f\"Found {len(input_dirs)} datasets with .tri files\")\n",
    "print(\"First 5:\", input_dirs[:5])\n",
    "\n",
    "# Trigger configuration for segmentation\n",
    "segment_by_triggers = {\n",
    "    \"2.0\": \"Video/Begin\",\n",
    "    \"3.0\": \"Video/End\",\n",
    "    \"0.0\": \"Video/TaskEnd\"\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# ÎàÑÎùΩÎêú ÌîºÌóòÏûêÎßå Ïû¨Ï≤òÎ¶¨ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú\n",
    "# Duration tolerance ÏàòÏ†ï ÌõÑ Zima/Splitscreen ÎùºÎ≤® ÌååÏùºÏù¥ ÏóÜÎäî 14Î™ÖÎßå Ïû¨Ï≤òÎ¶¨\n",
    "# Ìï¥Îãπ ÌîºÌóòÏûê: 40, 41, 42, 43, 44, 45, 46, 47, 50, 55, 56, 57, 109, 113\n",
    "# (sub-53ÏùÄ Ìä∏Î¶¨Í±∞ Í∏∞Î°ù Ïò§Î•òÎ°ú Î≥µÍµ¨ Î∂àÍ∞Ä)\n",
    "# \n",
    "# ÏÇ¨Ïö©Î≤ï: REPROCESS_MISSING_ONLY = TrueÎ°ú ÏÑ§Ï†ïÌïòÍ≥† Ï≤´ Î≤àÏß∏ ÏÖÄ Ïã§Ìñâ\n",
    "# ============================================================================\n",
    "REPROCESS_MISSING_ONLY = True  # TrueÎ°ú Î≥ÄÍ≤ΩÌïòÎ©¥ ÎàÑÎùΩÎêú ÌîºÌóòÏûêÎßå Ïû¨Ï≤òÎ¶¨\n",
    "\n",
    "missing_subjects = ['40', '41', '42', '43', '44', '45', '46', '47', '50', '55', '56', '57', '109', '113']\n",
    "\n",
    "if REPROCESS_MISSING_ONLY:\n",
    "    # ÎàÑÎùΩÎêú ÌîºÌóòÏûêÏùò input_dirÎßå ÌïÑÌÑ∞ÎßÅ\n",
    "    def get_subject_id(input_dir_path):\n",
    "        \"\"\"description.jsonÏóêÏÑú subject ID Ï∂îÏ∂ú\"\"\"\n",
    "        desc_files = list(input_dir_path.glob(\"*_description.json\"))\n",
    "        if desc_files:\n",
    "            with open(desc_files[0], 'r') as f:\n",
    "                desc = json.load(f)\n",
    "                return desc.get('subject', '')\n",
    "        return ''\n",
    "    \n",
    "    filtered_dirs = []\n",
    "    for rel_path in input_dirs:\n",
    "        full_path = input_base / rel_path\n",
    "        subj_id = get_subject_id(full_path)\n",
    "        # leading zeros Ï†úÍ±∞ÌïòÏó¨ ÎπÑÍµê (e.g., \"055\" -> \"55\")\n",
    "        subj_id_clean = subj_id.lstrip('0') or '0'\n",
    "        if subj_id_clean in missing_subjects or subj_id in missing_subjects:\n",
    "            filtered_dirs.append(rel_path)\n",
    "            print(f\"  ‚Üí Will reprocess: {rel_path} (subject {subj_id})\")\n",
    "    \n",
    "    input_dirs = filtered_dirs\n",
    "    print(f\"\\nüîÑ Reprocessing {len(input_dirs)} missing subjects only\")\n",
    "\n",
    "if 'INPUT_DIR' not in os.environ:\n",
    "    import subprocess\n",
    "\n",
    "    # Get notebook path\n",
    "    from IPython import get_ipython\n",
    "    ip = get_ipython()\n",
    "    if '__vsc_ipynb_file__' in ip.user_ns:\n",
    "        nb_path = ip.user_ns['__vsc_ipynb_file__']\n",
    "    else:\n",
    "        import ipynbname\n",
    "        nb_path = ipynbname.path()\n",
    "\n",
    "    # For each input directory, submit a subprocess to run nbconvert on this notebook\n",
    "    for target_input_dir in input_dirs:\n",
    "        date_folder = target_input_dir.split('/')[0]\n",
    "        session_folder = target_input_dir.split('/')[1]\n",
    "        full_input_dir = input_base / date_folder / session_folder\n",
    "\n",
    "        if not full_input_dir.exists():\n",
    "            print(f\"‚ö†Ô∏è  Skipping: {full_input_dir} does not exist.\")\n",
    "            continue\n",
    "\n",
    "        # Check if tri file is present\n",
    "        if not list(full_input_dir.glob(\"*.tri\")):\n",
    "            print(f\"‚ùå Skipping: No .tri file found in {full_input_dir}\")\n",
    "            continue\n",
    "\n",
    "        env = os.environ.copy()\n",
    "        env[\"INPUT_DIR\"] = str(full_input_dir)\n",
    "\n",
    "        output_name = f\"{session_folder}.ipynb\"\n",
    "\n",
    "        try:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Processing: {target_input_dir}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            # Use python -m jupyter instead of jupyter command directly\n",
    "            subprocess.run([\n",
    "                sys.executable, \"-m\", \"jupyter\", \"nbconvert\",\n",
    "                \"--to\", \"notebook\",\n",
    "                \"--execute\", str(nb_path),\n",
    "                \"--output\", output_name,\n",
    "                \"--output-dir\", str(output_base)\n",
    "            ], env=env, check=True)\n",
    "            print(f\"‚úÖ Success: {target_input_dir}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå Notebook execution failed for {target_input_dir}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing-header",
   "metadata": {},
   "source": [
    "# Preprocessing Pipeline\n",
    "\n",
    "Code adapted from https://mne.tools/stable/auto_tutorials/preprocessing/70_fnirs_processing.html\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "if 'INPUT_DIR' not in os.environ:\n",
    "    raise Exception('Missing INPUT_DIR variable! Need to run using nbconvert; see first code cell')\n",
    "\n",
    "from itertools import compress\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mne\n",
    "import mne_nirs\n",
    "\n",
    "input_dir = os.environ['INPUT_DIR']\n",
    "\n",
    "input_path = Path(input_dir)\n",
    "raw_intensity = mne.io.read_raw_nirx(input_path, verbose=True).load_data()\n",
    "\n",
    "subj = raw_intensity.info['subject_info']['his_id']\n",
    "\n",
    "# Extract session identifier from input directory (e.g., \"2025-03-12_001\")\n",
    "session_id = input_path.name\n",
    "\n",
    "# Check if accelerometer is available\n",
    "config_file = list(input_path.glob(\"*_config.json\"))[0]\n",
    "with open(config_file, 'r') as f:\n",
    "    config = json.load(f)\n",
    "has_accelerometer = config.get('use_accelerometer', False)\n",
    "print(f\"Dataset has accelerometer: {has_accelerometer}\")\n",
    "print(f\"Session ID: {session_id}\")\n",
    "\n",
    "# Create the full output directory\n",
    "output_dir = output_base / f'sub-{subj}'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-header",
   "metadata": {},
   "source": [
    "## Step 1: Compute Optical Density Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-density",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Plot raw intensity time series\n",
    "raw_intensity.plot(n_channels=len(raw_intensity.ch_names), duration=500, show_scrollbars=False)\n",
    "\n",
    "# Convert from raw intensity to optical density and plot\n",
    "raw_od = mne.preprocessing.nirs.optical_density(raw_intensity)\n",
    "raw_od.plot(n_channels=len(raw_od.ch_names), duration=500, show_scrollbars=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-header",
   "metadata": {},
   "source": [
    "## Step 2: Signal Quality Assessment (SCI)\n",
    "\n",
    "Identify and filter out channels with poor signal quality using the Scalp Coupling Index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sci-visualization",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize original SCI distribution\n",
    "sci = mne.preprocessing.nirs.scalp_coupling_index(raw_od)\n",
    "fig, ax = plt.subplots(layout=\"constrained\")\n",
    "ax.hist(sci)\n",
    "ax.set(xlabel=\"Scalp Coupling Index\", ylabel=\"Count\", xlim=[0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sci-filtering",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Filter out bad channels (SCI < 0.5)\n",
    "od = raw_od.copy()\n",
    "od.info[\"bads\"] = list(compress(od.ch_names, sci < 0.5))\n",
    "bad_channels = list(compress(od.ch_names, sci < 0.5))\n",
    "od.drop_channels(bad_channels)\n",
    "print(f\"Dropped bad channels based on SCI < 0.5: {bad_channels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sci-verification",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize new SCI distribution\n",
    "sci_clean = mne.preprocessing.nirs.scalp_coupling_index(od)\n",
    "fig, ax = plt.subplots(layout=\"constrained\")\n",
    "ax.hist(sci_clean, bins=20)\n",
    "ax.set(xlabel=\"Scalp Coupling Index (after removal of bad channels)\", ylabel=\"Count\", xlim=[0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-header",
   "metadata": {},
   "source": [
    "## Step 3: Signal Enhancement - Short Channel Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-channel-regression",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Regress out short channels to remove systemic noise\n",
    "od = mne_nirs.signal_enhancement.short_channel_regression(od)\n",
    "od.plot(n_channels=len(od.ch_names), duration=500, show_scrollbars=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-header",
   "metadata": {},
   "source": [
    "## Step 4: Motion Artifact Correction - TDDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tddr",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Apply Temporal Derivative Distribution Repair to remove motion artifacts\n",
    "od = mne.preprocessing.nirs.temporal_derivative_distribution_repair(od)\n",
    "od.plot(n_channels=len(od.ch_names), duration=500, show_scrollbars=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5-header",
   "metadata": {},
   "source": [
    "## Step 5: Motion Artifact Correction - Accelerometer (if available)\n",
    "\n",
    "TODO: Implement accelerometer-based motion correction for datasets with `use_accelerometer: true`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accelerometer-correction",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "if has_accelerometer:\n",
    "    print(\"‚ö†Ô∏è  Accelerometer data detected but correction not yet implemented\")\n",
    "    print(\"TODO: Add accelerometer-based motion artifact correction\")\n",
    "    # Future implementation here\n",
    "else:\n",
    "    print(\"No accelerometer data - skipping accelerometer-based correction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6-header",
   "metadata": {},
   "source": [
    "## Step 6: Convert to Haemoglobin Concentration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beer-lambert",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Convert from optical density to haemoglobin using Beer-Lambert law\n",
    "raw_haemo = mne.preprocessing.nirs.beer_lambert_law(od, ppf=0.1)\n",
    "raw_haemo = mne_nirs.channels.get_long_channels(raw_haemo)\n",
    "\n",
    "# Plot haemoglobin time series\n",
    "raw_haemo.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step7-header",
   "metadata": {},
   "source": [
    "## Step 7: Signal Filtering - Remove Heart Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filtering",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "haemo = raw_haemo.copy()\n",
    "haemo.filter(0.05, 0.7, h_trans_bandwidth=0.2, l_trans_bandwidth=0.02)\n",
    "\n",
    "# Visualize power spectral density before and after filtering\n",
    "for when, _haemo in dict(Before=raw_haemo, After=haemo).items():\n",
    "    fig = _haemo.compute_psd().plot(\n",
    "        average=True, amplitude=False, picks=\"data\", exclude=\"bads\"\n",
    "    )\n",
    "    fig.suptitle(f\"{when} filtering\", weight=\"bold\", size=\"x-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step8-header",
   "metadata": {},
   "source": [
    "## Step 8: Trigger-Based Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trigger-segmentation",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Rename triggers according to segment_by_triggers dictionary\n",
    "haemo.annotations.rename(segment_by_triggers)\n",
    "\n",
    "# Keep only the annotations we want to segment by\n",
    "wanted_annotations = list(segment_by_triggers.values())\n",
    "unwanted_annotations = np.nonzero(~np.isin(haemo.annotations.description, wanted_annotations))\n",
    "haemo.annotations.delete(unwanted_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hrf-shift",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "events, event_dict = mne.events_from_annotations(haemo)\n",
    "\n",
    "# Make each event unique\n",
    "event_desc = {v: k for k, v in event_dict.items()}\n",
    "original_event_ids = []\n",
    "for i, event in enumerate(events):\n",
    "    original_event_ids.append(event_desc[event[2]])\n",
    "    event[2] = i + 1\n",
    "\n",
    "# Apply HRF delay shift (6 seconds)\n",
    "shift_seconds_for_hrf_delay = 6.0\n",
    "events_hrf_shifted = mne.event.shift_time_events(\n",
    "    events, ids=None, tshift=shift_seconds_for_hrf_delay, sfreq=haemo.info[\"sfreq\"]\n",
    ")\n",
    "\n",
    "# Create annotations from shifted events\n",
    "annot_from_events = mne.annotations_from_events(\n",
    "    events=events_hrf_shifted,\n",
    "    sfreq=haemo.info[\"sfreq\"],\n",
    "    orig_time=haemo.info[\"meas_date\"],\n",
    "    first_samp=haemo.first_samp\n",
    ")\n",
    "\n",
    "# Set durations to span until next annotation\n",
    "mapping = {}\n",
    "for i, annot in enumerate(annot_from_events):\n",
    "    if i == len(annot_from_events) - 1:\n",
    "        continue\n",
    "    duration = annot_from_events.onset[i + 1] - annot[\"onset\"]\n",
    "    mapping[annot[\"description\"]] = duration\n",
    "annot_from_events.set_durations(mapping, verbose=True)\n",
    "\n",
    "# Restore original event IDs\n",
    "annot_from_events.rename(\n",
    "    {str(i + 1): original_event_ids[i] for i in range(len(original_event_ids))}\n",
    ")\n",
    "\n",
    "# Segment the data by annotations\n",
    "haemo_segments = haemo.crop_by_annotations(annot_from_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step9-header",
   "metadata": {},
   "source": [
    "## Step 9: Export to CSV\n",
    "\n",
    "Save full and segmented data using BIDS-like naming conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-csv",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Video duration to name mapping\n",
    "# NOTE: Original exact matching caused 15 subjects to miss video labels due to trigger timing variance\n",
    "# Affected subjects: sub-40~47, 50, 53, 55~57, 109, 113 (durations like 507, 504, 144 instead of 508, 145)\n",
    "# Additionally, 5 subjects have no source data: sub-1, 49, 63 (recorded as P63), 74, 77\n",
    "\n",
    "# Original exact matching (kept for reference):\n",
    "# video_duration_to_name = {\n",
    "#     508: 'Zima',\n",
    "#     145: 'Splitscreen'\n",
    "# }\n",
    "\n",
    "# Updated: Use tolerance-based matching to handle trigger timing variance\n",
    "def get_video_name(duration, tolerance=5):\n",
    "    \"\"\"Match video duration with tolerance for trigger timing variance.\"\"\"\n",
    "    if abs(duration - 508) <= tolerance:\n",
    "        return 'Zima'\n",
    "    elif abs(duration - 145) <= tolerance:\n",
    "        return 'Splitscreen'\n",
    "    return None\n",
    "\n",
    "# Export full preprocessed data with session ID\n",
    "haemo.to_data_frame().to_csv(output_dir / f\"sub-{subj}_ses-{session_id}_desc-preproc_haemo.csv\", index=False)\n",
    "annot_from_events.to_data_frame().to_csv(output_dir / f\"sub-{subj}_ses-{session_id}_annotations.csv\", index=False)\n",
    "\n",
    "# Export segmented data\n",
    "segment_index = 0\n",
    "for i, segment in enumerate(haemo_segments):\n",
    "    duration = int(annot_from_events.duration[i])\n",
    "    if duration > 0:\n",
    "        segment_index += 1\n",
    "        task_label = annot_from_events.description[i].split(\"/\")[0]\n",
    "        \n",
    "        # Check if this duration corresponds to a named video (with tolerance)\n",
    "        video_name = get_video_name(duration)\n",
    "        if video_name:\n",
    "            print(f\"Segment {segment_index}: duration={duration}s, video={video_name}\")\n",
    "            # Save with video name\n",
    "            segment.to_data_frame().to_csv(\n",
    "                output_dir / f\"sub-{subj}_ses-{session_id}_task-{task_label}_label-{video_name}_haemo.csv\",\n",
    "                index=False\n",
    "            )\n",
    "        \n",
    "        # Always save with acq number and duration\n",
    "        segment.to_data_frame().to_csv(\n",
    "            output_dir / f\"sub-{subj}_ses-{session_id}_task-{task_label}_acq-{segment_index}_dur-{duration}_desc-preproc_haemo.csv\",\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "print(f\"\\n‚úÖ Preprocessing complete for subject {subj}, session {session_id}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Exported {segment_index} segments\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
