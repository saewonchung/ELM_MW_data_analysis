{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use **only this first cell** to execute this notebook automatically for multiple subjects in a row (see `input_dirs`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 2895329 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-02-26_002.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping: No .tri file found in /Users/saewonchung/Desktop/fNIRS pilot analysis/Data/2025-02-26/2025-02-26_003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 2937627 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-03-03_001.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 3141887 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-03-05_001.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 3434907 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-03-05_002.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping: No .tri file found in /Users/saewonchung/Desktop/fNIRS pilot analysis/Data/2025-03-05/2025-03-05_003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 3208353 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-03-07_001.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 2990772 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-03-12_001.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 3114932 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-03-12_002.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 2956559 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-03-13_001.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 1987359 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-03-14_001.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 2627921 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-03-24_001.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 3117309 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-03-26_002.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 2881791 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-03-31_001.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping: No .tri file found in /Users/saewonchung/Desktop/fNIRS pilot analysis/Data/2025-03-31/2025-03-31_002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 3551458 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-03-31_003.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 3301068 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-04-02_001.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 2935252 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-04-02_003.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 2784838 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-04-07_001.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 3503003 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-04-08_001.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 3222383 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-04-09_002.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 4061965 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-04-09_003.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 3030499 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-04-11_001.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 2251511 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-04-11_002.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 3096499 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-04-15_001.ipynb\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "input_base = Path(\"/Users/saewonchung/Desktop/fNIRS pilot analysis/Data\")\n",
    "output_base = Path(\"/Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed\")\n",
    "\n",
    "# Get notebook path\n",
    "from IPython import get_ipython\n",
    "ip = get_ipython()\n",
    "if '__vsc_ipynb_file__' in ip.user_ns:\n",
    "    nb_path = ip.user_ns['__vsc_ipynb_file__']\n",
    "else:\n",
    "    import ipynbname\n",
    "    nb_path = ipynbname.path()\n",
    "\n",
    "input_dirs = [\n",
    "    '2025-02-26_002',\n",
    "    '2025-02-26_003',\n",
    "    '2025-03-03_001',\n",
    "    '2025-03-05_001',\n",
    "    '2025-03-05_002',\n",
    "    '2025-03-05_003',\n",
    "    '2025-03-07_001',\n",
    "    '2025-03-12_001',\n",
    "    '2025-03-12_002',\n",
    "    '2025-03-13_001',\n",
    "    '2025-03-14_001',\n",
    "    '2025-03-24_001',\n",
    "    '2025-03-26_002',\n",
    "    '2025-03-31_001',\n",
    "    '2025-03-31_002',\n",
    "    '2025-03-31_003',\n",
    "    '2025-04-02_001',\n",
    "    '2025-04-02_003',\n",
    "    '2025-04-07_001',\n",
    "    '2025-04-08_001',\n",
    "    '2025-04-09_002',\n",
    "    '2025-04-09_003',\n",
    "    '2025-04-11_001',\n",
    "    '2025-04-11_002',\n",
    "    '2025-04-15_001'\n",
    "]\n",
    "\n",
    "for target_input_dir in input_dirs:\n",
    "    date_folder = target_input_dir.split('_')[0]\n",
    "    full_input_dir = input_base / date_folder / target_input_dir\n",
    "\n",
    "    if not full_input_dir.exists():\n",
    "        print(f\"⚠️ Skipping: {full_input_dir} does not exist.\")\n",
    "        continue\n",
    "\n",
    "    # Check if tri file is present\n",
    "    if not list(full_input_dir.glob(\"*.tri\")):\n",
    "        print(f\"❌ Skipping: No .tri file found in {full_input_dir}\")\n",
    "        continue\n",
    "\n",
    "    env = os.environ.copy()\n",
    "    env[\"INPUT_DIR\"] = str(full_input_dir)\n",
    "\n",
    "    output_name = f\"{target_input_dir}.ipynb\"\n",
    "\n",
    "    try:\n",
    "        subprocess.run([\n",
    "            \"jupyter\", \"nbconvert\",\n",
    "            \"--to\", \"notebook\",\n",
    "            \"--execute\", str(nb_path),\n",
    "            \"--output\", output_name,\n",
    "            \"--output-dir\", str(output_base)\n",
    "        ], env=env, check=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"❌ Notebook execution failed for {target_input_dir}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code samples borrowed from https://mne.tools/stable/auto_tutorials/preprocessing/70_fnirs_processing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "from pathlib import Path\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mne\n",
    "\n",
    "if 'INPUT_DIR' not in os.environ:\n",
    "    raise 'Missing INPUT_DIR variable! Need to run using nbconvert; see cell near top'\n",
    "\n",
    "input_dir = os.environ['INPUT_DIR']\n",
    "\n",
    "input_path = Path(input_base) / input_dir.split('_')[0] / input_dir\n",
    "raw_intensity = mne.io.read_raw_nirx(input_path, verbose=True)\n",
    "raw_intensity.load_data()\n",
    "\n",
    "subj = raw_intensity.info['subject_info']['his_id']\n",
    "\n",
    "# Create the full output directory\n",
    "output_dir = output_base / f'sub-{subj}'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects_dir = mne.datasets.sample.data_path() / \"subjects\"\n",
    "\n",
    "# brain = mne.viz.Brain(\n",
    "#     \"fsaverage\", subjects_dir=subjects_dir, background=\"w\", cortex=\"0.5\"\n",
    "# )\n",
    "# brain.add_sensors(\n",
    "#     raw_intensity.info,\n",
    "#     trans=\"fsaverage\",\n",
    "#     fnirs=[\"channels\", \"pairs\", \"sources\", \"detectors\"],\n",
    "# )\n",
    "# brain.show_view(azimuth=20, elevation=60, distance=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting channels appropriate for detecting neural responses\n",
    "picks = mne.pick_types(raw_intensity.info, meg=False, fnirs=True)\n",
    "dists = mne.preprocessing.nirs.source_detector_distances(\n",
    "    raw_intensity.info, picks=picks\n",
    ")\n",
    "raw_intensity.pick(picks[dists > 0.01])\n",
    "raw_intensity.plot(\n",
    "   n_channels=len(raw_intensity.ch_names), duration=500, show_scrollbars=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting from raw intensity to optical density\n",
    "raw_od = mne.preprocessing.nirs.optical_density(raw_intensity)\n",
    "raw_od.plot(n_channels=len(raw_od.ch_names), duration=500, show_scrollbars=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying channels with poor signal quality (SCI)\n",
    "sci = mne.preprocessing.nirs.scalp_coupling_index(raw_od)\n",
    "fig, ax = plt.subplots(layout=\"constrained\")\n",
    "ax.hist(sci)\n",
    "ax.set(xlabel=\"Scalp Coupling Index\", ylabel=\"Count\", xlim=[0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out bad channels\n",
    "raw_od.info[\"bads\"] = list(compress(raw_od.ch_names, sci < 0.5))\n",
    "bad_channels = list(compress(raw_od.ch_names, sci < 0.5))\n",
    "raw_od.drop_channels(bad_channels)\n",
    "print(f\"Dropped bad channels based on SCI < 0.5: {bad_channels}\")\n",
    "\n",
    "# Updated Scalp Coupling Index (SCI)\n",
    "sci_clean = mne.preprocessing.nirs.scalp_coupling_index(raw_od)\n",
    "\n",
    "# Visualize updated SCI distribution\n",
    "fig, ax = plt.subplots(layout=\"constrained\")\n",
    "ax.hist(sci_clean, bins=20)\n",
    "ax.set(xlabel=\"Scalp Coupling Index (after removal)\", ylabel=\"Count\", xlim=[0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artifact correction\n",
    "\n",
    "# Applying temporal derivative distribution repair\n",
    "from mne.preprocessing.nirs import temporal_derivative_distribution_repair\n",
    "\n",
    "# Apply TDDR to remove motion artifacts\n",
    "raw_od = temporal_derivative_distribution_repair(raw_od)\n",
    "\n",
    "# Optionally, plot to inspect effect of correction\n",
    "raw_od.plot(n_channels=len(raw_od.ch_names), duration=500, show_scrollbars=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting from optical density to haemoglobin\n",
    "raw_haemo = mne.preprocessing.nirs.beer_lambert_law(raw_od, ppf=0.1)\n",
    "\n",
    "# Built-in plotting methods for Raw objects\n",
    "raw_haemo.plot()\n",
    "# raw_haemo.plot(picks=\"hbo\")\n",
    "# raw_haemo.plot(picks=\"hbr\")\n",
    "\n",
    "# raw_haemo.annotations\n",
    "raw_haemo.annotations.rename(\n",
    "    {\"2.0\": \"Video/Begin\",\n",
    "     \"3.0\": \"Video/End\",\n",
    "     \"0.0\": \"Video/TaskEnd\"} # NOTE: maybe this should actually be \"Video/TaskBegin\"\n",
    "     # when we want to use the other triggers, we should rename them here as well\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now only keep the annotations that we want to split the time series by\n",
    "unwanted = np.nonzero(~(\n",
    "    (raw_haemo.annotations.description == \"Video/Begin\") |\n",
    "    (raw_haemo.annotations.description == \"Video/End\") |\n",
    "    (raw_haemo.annotations.description == \"Video/TaskEnd\")\n",
    "))\n",
    "raw_haemo.annotations.delete(unwanted)\n",
    "\n",
    "# raw_haemo.plot(n_channels=len(raw_haemo.ch_names), duration=raw_haemo.n_times, show_scrollbars=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing heart rate from signal\n",
    "raw_haemo_unfiltered = raw_haemo.copy()\n",
    "raw_haemo.filter(0.05, 0.7, h_trans_bandwidth=0.2, l_trans_bandwidth=0.02)\n",
    "for when, _raw in dict(Before=raw_haemo_unfiltered, After=raw_haemo).items():\n",
    "    fig = _raw.compute_psd().plot(\n",
    "        average=True, amplitude=False, picks=\"data\", exclude=\"bads\"\n",
    "    )\n",
    "    fig.suptitle(f\"{when} filtering\", weight=\"bold\", size=\"x-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short channel correction (Physio correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Analysis for motion correction\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "# hbo_data = raw_haemo.get_data(picks=\"hbo\")\n",
    "# pca = PCA()\n",
    "# components = pca.fit_transform(hbo_data.T)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(pca.explained_variance_ratio_ * 100)\n",
    "# plt.xlabel('Component')\n",
    "# plt.ylabel('Variance explained (%)')\n",
    "# plt.title('PCA of HbO signal')\n",
    "# plt.show()\n",
    "\n",
    "# n_remove = 1  # Try 1–2 first\n",
    "# components[:, :n_remove] = 0  # Zero out motion components\n",
    "# denoised_data = pca.inverse_transform(components).T\n",
    "\n",
    "# raw_clean = raw_haemo.copy()\n",
    "# hbo_indices = mne.pick_types(raw_clean.info, fnirs=\"hbo\")\n",
    "# raw_clean._data[hbo_indices] = denoised_data\n",
    "\n",
    "# raw_haemo.plot(picks=\"hbo\", title=\"Before PCA\")\n",
    "# raw_clean.plot(picks=\"hbo\", title=\"After PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch extraction and drop epochs we need to drop\n",
    "\n",
    "# events, event_dict = mne.events_from_annotations(raw_haemo)\n",
    "# fig = mne.viz.plot_events(events, event_id=event_dict, sfreq=raw_haemo.info[\"sfreq\"])\n",
    "\n",
    "# reject_criteria = dict(hbo=80e-6)\n",
    "# tmin, tmax = -5, 15\n",
    "\n",
    "# epochs = mne.Epochs(\n",
    "#    raw_haemo,\n",
    "#    events,\n",
    "#    event_id=event_dict,\n",
    "#    tmin=tmin,\n",
    "#    tmax=tmax,\n",
    "#    reject=reject_criteria,\n",
    "#    reject_by_annotation=True,\n",
    "#    proj=True,\n",
    "#    baseline=(None, 0),\n",
    "#    preload=True,\n",
    "#    detrend=None,\n",
    "#    verbose=True,\n",
    "# )\n",
    "# epochs.plot_drop_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding repeated events for epochs\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# events_df = pd.DataFrame(events, columns=[\"sample\", \"previous\", \"event_id\"])\n",
    "# duplicated_events = events_df[events_df.duplicated(subset=\"sample\", keep=False)]\n",
    "\n",
    "# print(\"Repeated events:\")\n",
    "# print(duplicated_events.sort_values(\"sample\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events, event_dict = mne.events_from_annotations(raw_haemo)\n",
    "# fig = mne.viz.plot_events(events, event_id=event_dict, sfreq=raw_haemo.info[\"sfreq\"])\n",
    "\n",
    "# here we would drop epochs we need to drop? (https://mne.tools/stable/auto_tutorials/preprocessing/70_fnirs_processing.html#extract-epochs)\n",
    "\n",
    "# events is a 2D array, with each row being [t, ?, id]\n",
    "# let's replace id with 1,2,3,4,etc so that each event is treated uniquely\n",
    "event_desc = { v: k for k, v in event_dict.items() }\n",
    "original_event_ids = []\n",
    "for i, event in enumerate(events):\n",
    "    original_event_ids.append(event_desc[event[2]])\n",
    "    event[2] = i + 1\n",
    "\n",
    "shift_seconds_for_hrf_delay = 6.0\n",
    "events_hrf_shifted = mne.event.shift_time_events(events, ids=None, tshift=shift_seconds_for_hrf_delay, sfreq=raw_haemo.info[\"sfreq\"])\n",
    "\n",
    "annot_from_events = mne.annotations_from_events(\n",
    "    events=events_hrf_shifted,\n",
    "    sfreq=raw_haemo.info[\"sfreq\"],\n",
    "    orig_time=raw_haemo.info[\"meas_date\"],\n",
    "    first_samp=raw_haemo.first_samp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each annotation, set the duration to be the time until the next annotation's onset\n",
    "# map from annotation description to new duration\n",
    "mapping = {}\n",
    "for i, annot in enumerate(annot_from_events):\n",
    "    if i == len(annot_from_events) - 1:\n",
    "        continue\n",
    "    duration = annot_from_events.onset[i + 1] - annot[\"onset\"]\n",
    "    mapping[annot[\"description\"]] = duration\n",
    "annot_from_events.set_durations(mapping, verbose=True)\n",
    "\n",
    "# now give back the original event ids\n",
    "annot_from_events.rename(\n",
    "    {str(i + 1): original_event_ids[i] for i in range(len(original_event_ids))}\n",
    ")\n",
    "\n",
    "raw_haemo_segments = raw_haemo.crop_by_annotations(annot_from_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now save all data as csvs\n",
    "\n",
    "video_duration_to_name = {\n",
    "    508: 'Zima',\n",
    "    145: 'Splitscreen'\n",
    "}\n",
    "\n",
    "for i, segment in enumerate(raw_haemo_segments):\n",
    "    # label = annot_from_events.description[i].replace(\"/\", \"-\")\n",
    "    # print(i, label, annot_from_events.onset[i], annot_from_events.duration[i])\n",
    "    # #segment.to_data_frame().to_csv(f\"SC-segment_{i}_{label}.csv\")\n",
    "    # segment.to_data_frame().to_csv(output_dir / f\"SC-segment_{i}_{label}.csv\", index=False)\n",
    "    if int(annot_from_events.duration[i]) in video_duration_to_name:\n",
    "        task_label = annot_from_events.description[i].split(\"/\")[0]\n",
    "        video_name = video_duration_to_name[int(annot_from_events.duration[i])]\n",
    "        print(annot_from_events.duration[i], video_name)\n",
    "        segment.to_data_frame().to_csv(output_dir / f\"sub-{subj}_task-{task_label}_label-{video_name}_haemo.csv\", index=False)\n",
    "\n",
    "# # raw_haemo.to_data_frame()\n",
    "# #raw_haemo.to_data_frame().to_csv(\"SC-raw_haemo.csv\")\n",
    "raw_haemo.to_data_frame().to_csv(output_dir / f\"sub-{subj}_rawhaemo.csv\", index=False)\n",
    "\n",
    "# # annot_from_events.to_data_frame()\n",
    "# #annot_from_events.to_data_frame().to_csv(\"SC-annotations.csv\")\n",
    "annot_from_events.to_data_frame().to_csv(output_dir / f\"sub-{subj}_annotations.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(raw_haemo_segments)\n",
    "# print(len(raw_haemo_segments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
