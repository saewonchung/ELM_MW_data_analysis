{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53f8b723",
   "metadata": {},
   "source": [
    "use **only this first code cell** to execute this notebook automatically for multiple subjects in a row (see `input_dirs`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804fcc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 2895329 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-02-26_002.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping: No .tri file found in /Users/saewonchung/Desktop/fNIRS pilot analysis/Data/2025-02-26/2025-02-26_003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 2937627 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-03-03_001.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 3141887 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-03-05_001.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 3434907 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-03-05_002.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping: No .tri file found in /Users/saewonchung/Desktop/fNIRS pilot analysis/Data/2025-03-05/2025-03-05_003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 3208353 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-03-07_001.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 2990772 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-03-12_001.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 3114932 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-03-12_002.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 2956559 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-03-13_001.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 1987359 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-03-14_001.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 2627921 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-03-24_001.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 3117309 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-03-26_002.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 2881791 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-03-31_001.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping: No .tri file found in /Users/saewonchung/Desktop/fNIRS pilot analysis/Data/2025-03-31/2025-03-31_002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 3551458 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-03-31_003.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 3301068 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-04-02_001.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 2935252 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-04-02_003.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 2784838 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-04-07_001.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 3503003 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-04-08_001.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 3222383 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-04-09_002.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 4061965 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-04-09_003.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 3030499 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-04-11_001.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 2251511 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-04-11_002.ipynb\n",
      "[NbConvertApp] Converting notebook /Users/saewonchung/Desktop/fNIRS pilot analysis/nirx-to-csv_JZ-parameterized-4172025_SC.ipynb to notebook\n",
      "[NbConvertApp] Writing 3096499 bytes to /Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed/2025-04-15_001.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "input_base = Path(\"/Users/saewonchung/Desktop/fNIRS pilot analysis/Data\")\n",
    "output_base = Path(\"/Users/saewonchung/Desktop/fNIRS pilot analysis/Data_preprocessed\")\n",
    "\n",
    "input_dirs = [\n",
    "    '2025-02-26_002',\n",
    "    '2025-02-26_003',\n",
    "    '2025-03-03_001',\n",
    "    '2025-03-05_001',\n",
    "    '2025-03-05_002',\n",
    "    '2025-03-05_003',\n",
    "    '2025-03-07_001',\n",
    "    '2025-03-12_001',\n",
    "    '2025-03-12_002',\n",
    "    '2025-03-13_001',\n",
    "    '2025-03-14_001',\n",
    "    '2025-03-24_001',\n",
    "    '2025-03-26_002',\n",
    "    '2025-03-31_001',\n",
    "    '2025-03-31_002',\n",
    "    '2025-03-31_003',\n",
    "    '2025-04-02_001',\n",
    "    '2025-04-02_003',\n",
    "    '2025-04-07_001',\n",
    "    '2025-04-08_001',\n",
    "    '2025-04-09_002',\n",
    "    '2025-04-09_003',\n",
    "    '2025-04-11_001',\n",
    "    '2025-04-11_002',\n",
    "    '2025-04-15_001'\n",
    "]\n",
    "\n",
    "# expects name like \"TaskName/EventInTask\"\n",
    "segment_by_triggers = {\n",
    "    \"2.0\": \"Video/Begin\",\n",
    "    \"3.0\": \"Video/End\",\n",
    "    \"0.0\": \"Video/TaskEnd\"\n",
    "}\n",
    "\n",
    "if 'INPUT_DIR' not in os.environ:\n",
    "    import subprocess\n",
    "\n",
    "    # Get notebook path\n",
    "    from IPython import get_ipython\n",
    "    ip = get_ipython()\n",
    "    if '__vsc_ipynb_file__' in ip.user_ns:\n",
    "        nb_path = ip.user_ns['__vsc_ipynb_file__']\n",
    "    else:\n",
    "        import ipynbname\n",
    "        nb_path = ipynbname.path()\n",
    "\n",
    "    # For each, input directory, submit a subprocess to run nbconvert on this notebook\n",
    "    for target_input_dir in input_dirs:\n",
    "        date_folder = target_input_dir.split('_')[0]\n",
    "        full_input_dir = input_base / date_folder / target_input_dir\n",
    "\n",
    "        if not full_input_dir.exists():\n",
    "            print(f\"⚠️ Skipping: {full_input_dir} does not exist.\")\n",
    "            continue\n",
    "\n",
    "        # Check if tri file is present\n",
    "        if not list(full_input_dir.glob(\"*.tri\")):\n",
    "            print(f\"❌ Skipping: No .tri file found in {full_input_dir}\")\n",
    "            continue\n",
    "\n",
    "        env = os.environ.copy()\n",
    "        env[\"INPUT_DIR\"] = str(full_input_dir)\n",
    "\n",
    "        output_name = f\"{target_input_dir}.ipynb\"\n",
    "\n",
    "        try:\n",
    "            subprocess.run([\n",
    "                \"jupyter\", \"nbconvert\",\n",
    "                \"--to\", \"notebook\",\n",
    "                \"--execute\", str(nb_path),\n",
    "                \"--output\", output_name,\n",
    "                \"--output-dir\", str(output_base)\n",
    "            ], env=env, check=True)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"❌ Notebook execution failed for {target_input_dir}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a106d779",
   "metadata": {},
   "source": [
    "code samples borrowed from https://mne.tools/stable/auto_tutorials/preprocessing/70_fnirs_processing.html\n",
    "\n",
    "# To-dos\n",
    "\n",
    "- [ ] run through this notebook on CICI data and check for any issues\n",
    "- [ ] add dropping of bad epochs (= events? event starts?) (https://mne.tools/stable/auto_tutorials/preprocessing/70_fnirs_processing.html#extract-epochs) and add in verification of consistency in response across trials, considering the epochs (https://mne.tools/stable/auto_tutorials/preprocessing/70_fnirs_processing.html#view-consistency-of-responses-across-trials) (re epochs, also see https://mne.tools/stable/auto_tutorials/epochs/index.html) (re visualizing average/group-level responses for epochs, can also see https://mne.tools/mne-nirs/stable/auto_examples/general/plot_15_waveform.html and https://mne.tools/mne-nirs/stable/auto_examples/general/plot_16_waveform_group.html)\n",
    "- [ ] somehow implement the advice of https://rachitsingh.com/collaborating-jupyter/ (chiefly: don't mutate across cells) while making it easy to disable/reenable processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5085443f",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cffbca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'INPUT_DIR' not in os.environ:\n",
    "    raise 'Missing INPUT_DIR variable! Need to run using nbconvert; see first code cell'\n",
    "\n",
    "from itertools import compress\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mne\n",
    "import mne_nirs\n",
    "\n",
    "input_dir = os.environ['INPUT_DIR']\n",
    "\n",
    "input_path = Path(input_base) / input_dir.split('_')[0] / input_dir\n",
    "raw_intensity = mne.io.read_raw_nirx(input_path, verbose=True).load_data()\n",
    "\n",
    "subj = raw_intensity.info['subject_info']['his_id']\n",
    "\n",
    "# Create the full output directory\n",
    "output_dir = output_base / f'sub-{subj}'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d47f764",
   "metadata": {},
   "source": [
    "Compute optical density timeseries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a4cf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot raw intensity time series\n",
    "raw_intensity.plot(n_channels=len(raw_intensity.ch_names), duration=500, show_scrollbars=False)\n",
    "\n",
    "# Convert from raw intensity to optical density and plot\n",
    "raw_od = mne.preprocessing.nirs.optical_density(raw_intensity)\n",
    "raw_od.plot(n_channels=len(raw_od.ch_names), duration=500, show_scrollbars=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52c8071",
   "metadata": {},
   "source": [
    "Identify and filter out channels with poor signal quality (using SCI, scalp coupling index).\n",
    "\n",
    "For another approach (Peak Power Metric), see https://mne.tools/mne-nirs/stable/auto_examples/general/plot_22_quality.html#sphx-glr-auto-examples-general-plot-22-quality-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9b21ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize original SCI distribution\n",
    "sci = mne.preprocessing.nirs.scalp_coupling_index(raw_od)\n",
    "fig, ax = plt.subplots(layout=\"constrained\")\n",
    "ax.hist(sci)\n",
    "ax.set(xlabel=\"Scalp Coupling Index\", ylabel=\"Count\", xlim=[0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff3cdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out bad channels\n",
    "od = raw_od.copy()\n",
    "od.info[\"bads\"] = list(compress(od.ch_names, sci < 0.5))\n",
    "bad_channels = list(compress(od.ch_names, sci < 0.5))\n",
    "od.drop_channels(bad_channels)\n",
    "print(f\"Dropped bad channels based on SCI < 0.5: {bad_channels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6798508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize new SCI distribution\n",
    "sci_clean = mne.preprocessing.nirs.scalp_coupling_index(od)\n",
    "fig, ax = plt.subplots(layout=\"constrained\")\n",
    "ax.hist(sci_clean, bins=20)\n",
    "ax.set(xlabel=\"Scalp Coupling Index (after removal of bad channels)\", ylabel=\"Count\", xlim=[0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66866102",
   "metadata": {},
   "source": [
    "Signal enhancement - Regress out short channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e9a252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: confirm that should drop bad channels first. seems prudent to drop bad short channels rather than regressing them out\n",
    "# ref https://mne.tools/mne-nirs/stable/auto_examples/general/plot_20_enhance.html#apply-short-channel-correction\n",
    "od = mne_nirs.signal_enhancement.short_channel_regression(od)\n",
    "od.plot(n_channels=len(od.ch_names), duration=500, show_scrollbars=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f98599",
   "metadata": {},
   "source": [
    "Motion artifact correction - Apply TDDR (conflicts with other motion-correction steps?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0172a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply TDDR to remove motion artifacts\n",
    "od = mne.preprocessing.nirs.temporal_derivative_distribution_repair(od)\n",
    "od.plot(n_channels=len(od.ch_names), duration=500, show_scrollbars=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dbd61c",
   "metadata": {},
   "source": [
    "Motion artifact correction - use accelerometer data (conflicts with other motion-correction steps?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aea25a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accelerometer-based motion artifact correction\n",
    "# TODO - should occur at this step, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb0d64c",
   "metadata": {},
   "source": [
    "Compute haemoglobin time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b6fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting from optical density to haemoglobin\n",
    "raw_haemo = mne.preprocessing.nirs.beer_lambert_law(od, ppf=0.1)\n",
    "raw_haemo = mne_nirs.channels.get_long_channels(raw_haemo)\n",
    "\n",
    "# Built-in plotting methods for Raw objects\n",
    "raw_haemo.plot()\n",
    "# raw_haemo.plot(picks=\"hbo\")\n",
    "# raw_haemo.plot(picks=\"hbr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bee49df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we would drop epochs we need to drop? (https://mne.tools/stable/auto_tutorials/preprocessing/70_fnirs_processing.html#extract-epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33c6273",
   "metadata": {},
   "source": [
    "Signal filtering - removing heart rate from signal\n",
    "\n",
    "If interested, could also consider Mayer Wave Parametrisation ([example code](https://mne.tools/mne-nirs/stable/auto_examples/general/plot_40_mayer.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1c65ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "haemo = raw_haemo.copy()\n",
    "haemo.filter(0.05, 0.7, h_trans_bandwidth=0.2, l_trans_bandwidth=0.02)\n",
    "for when, _haemo in dict(Before=raw_haemo, After=haemo).items():\n",
    "    fig = _haemo.compute_psd().plot(\n",
    "        average=True, amplitude=False, picks=\"data\", exclude=\"bads\"\n",
    "    )\n",
    "    fig.suptitle(f\"{when} filtering\", weight=\"bold\", size=\"x-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa20cc3",
   "metadata": {},
   "source": [
    "Motion artifact correction - PCA to filter out motion artifacts (conflicts with other motion-correction steps?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950267b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Analysis for motion correction\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "# hbo_data = haemo.get_data(picks=\"hbo\")\n",
    "# pca = PCA()\n",
    "# components = pca.fit_transform(hbo_data.T)\n",
    "\n",
    "# plt.plot(pca.explained_variance_ratio_ * 100)\n",
    "# plt.xlabel('Component')\n",
    "# plt.ylabel('Variance explained (%)')\n",
    "# plt.title('PCA of HbO signal')\n",
    "# plt.show()\n",
    "\n",
    "# n_remove = 1  # Try 1–2 first\n",
    "# components[:, :n_remove] = 0  # Zero out motion components\n",
    "# denoised_data = pca.inverse_transform(components).T\n",
    "\n",
    "# haemo_beforepca = haemo.copy()\n",
    "# hbo_indices = mne.pick_types(haemo.info, fnirs=\"hbo\")\n",
    "# haemo._data[hbo_indices] = denoised_data\n",
    "\n",
    "# haemo_beforepca.plot(picks=\"hbo\", title=\"Before PCA\")\n",
    "# haemo.plot(picks=\"hbo\", title=\"After PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b538ed25",
   "metadata": {},
   "source": [
    "Segment time series by the triggers named in `segment_by_triggers`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5954d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "haemo.annotations.rename(segment_by_triggers)\n",
    "\n",
    "# now only keep the annotations that we want to split the time series by\n",
    "wanted_annotations = list(segment_by_triggers.values())\n",
    "unwanted_annotations = np.nonzero(~np.isin(haemo.annotations.description, wanted_annotations))\n",
    "haemo.annotations.delete(unwanted_annotations)\n",
    "\n",
    "# haemo.plot(n_channels=len(haemo.ch_names), duration=haemo.n_times, show_scrollbars=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad15fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "events, event_dict = mne.events_from_annotations(haemo)\n",
    "# fig = mne.viz.plot_events(events, event_id=event_dict, sfreq=haemo.info[\"sfreq\"])\n",
    "\n",
    "# events is a 2D array, with each row being [t, ?, id]\n",
    "# let's replace id with 1,2,3,4,etc so that each event is treated uniquely\n",
    "event_desc = { v: k for k, v in event_dict.items() }\n",
    "original_event_ids = []\n",
    "for i, event in enumerate(events):\n",
    "    original_event_ids.append(event_desc[event[2]])\n",
    "    event[2] = i + 1\n",
    "\n",
    "shift_seconds_for_hrf_delay = 6.0\n",
    "events_hrf_shifted = mne.event.shift_time_events(events, ids=None, tshift=shift_seconds_for_hrf_delay, sfreq=haemo.info[\"sfreq\"])\n",
    "\n",
    "annot_from_events = mne.annotations_from_events(\n",
    "    events=events_hrf_shifted,\n",
    "    sfreq=haemo.info[\"sfreq\"],\n",
    "    orig_time=haemo.info[\"meas_date\"],\n",
    "    first_samp=haemo.first_samp\n",
    ")\n",
    "\n",
    "# for each annotation, set the duration to be the time until the next annotation's onset\n",
    "# map from annotation description to new duration\n",
    "mapping = {}\n",
    "for i, annot in enumerate(annot_from_events):\n",
    "    if i == len(annot_from_events) - 1:\n",
    "        continue\n",
    "    duration = annot_from_events.onset[i + 1] - annot[\"onset\"]\n",
    "    mapping[annot[\"description\"]] = duration\n",
    "annot_from_events.set_durations(mapping, verbose=True)\n",
    "\n",
    "# now give back the original event ids\n",
    "annot_from_events.rename(\n",
    "    {str(i + 1): original_event_ids[i] for i in range(len(original_event_ids))}\n",
    ")\n",
    "\n",
    "haemo_segments = haemo.crop_by_annotations(annot_from_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a098e7",
   "metadata": {},
   "source": [
    "Now save full and segmented data as csvs, (loosely) using BIDS conventions for file names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1f1312",
   "metadata": {},
   "outputs": [],
   "source": [
    "haemo.to_data_frame().to_csv(output_dir / f\"sub-{subj}_desc-preproc_haemo.csv\", index=False)\n",
    "annot_from_events.to_data_frame().to_csv(output_dir / f\"sub-{subj}_annotations.csv\", index=False)\n",
    "\n",
    "segment_index = 0\n",
    "for i, segment in enumerate(haemo_segments):\n",
    "    duration = int(annot_from_events.duration[i])\n",
    "    if duration > 0:\n",
    "        segment_index += 1\n",
    "        task_label = annot_from_events.description[i].split(\"/\")[0]\n",
    "        segment.to_data_frame().to_csv(output_dir / f\"sub-{subj}_task-{task_label}_acq-{segment_index}_dur-{duration}_desc-preproc_haemo.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
